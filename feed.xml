<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jiayuww.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jiayuww.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-26T04:12:51+00:00</updated><id>https://jiayuww.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Unveiling the Potential of Language Models in LaTeX Synthesis</title><link href="https://jiayuww.github.io/blog/2023/text2latex/" rel="alternate" type="text/html" title="Unveiling the Potential of Language Models in LaTeX Synthesis"/><published>2023-12-20T10:18:00+00:00</published><updated>2023-12-20T10:18:00+00:00</updated><id>https://jiayuww.github.io/blog/2023/text2latex</id><content type="html" xml:base="https://jiayuww.github.io/blog/2023/text2latex/"><![CDATA[<section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="https://github.com/jiayuww/text2latex/blob/main/cs703_Text2LaTeX_JiayuWang.pdf" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Paper</span> </a> </span> <span class="link-block"> <a href="https://github.com/jiayuww/text2latex" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span> <span class="link-block"> <a href="https://github.com/jiayuww/text2latex" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="far fa-images"></i> </span> <span>Data</span> </a> </div> </div> </div> </div> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <div class="content has-text-justified"> <p> LaTeX has been one of the de facto standards for the communication and publication of scientific documents. Generating LaTeX code that follows users' intentions is an important task in the digital age, which requires the model to understand user inputs presented in various formats and correctly generate grammatically correct code that can be rendered. Traditional rule-based methods, despite their high accuracy, often limit input to a specific format to generate valid LaTeX code, which substantially reduces the model's applicability. In reality, the same LaTeX expression might be represented in multiple formats such as images or textual descriptions. </p> <p> In this work, we investigate LaTeX program synthesis with foundation models that allow flexible inputs represented in texts or images. We propose to use a tree-based approach to generate LaTeX expressions with various diversity and scale. We observe that larger pre-trained models leads to improved performance. With an appropriate model, the prediction accuracy improves with an increase in both the size and complexity of the training set. Specifically, training on just 2500 samples (with a depth of 3) yields promising results, achieving 99.0% accuracy on test sets with expressions of depth-2, compared to 88.5% accuracy when training with the same sample size but a depth-2 input. This suggests the effectiveness of learning-based LaTeX code generation. </p> </div> </div> </div> <section class="section" id="Main Results"> <h2 class="title">Main Results</h2> <div class="row mt-3"> <div class="col-md-6 mt-3"> <figure> <picture> <img src="/assets/img/text2latex/paramsize_acc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Test accuracy comparison of backbone models CodeGen-350M-mono vs CodeGen-2B-mono. A larger pre-trained model yields a higher accuracy rate. </div> </div> <div class="col-md-6 mt-3"> <figure> <picture> <img src="/assets/img/text2latex/trainingsize_acc_red.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Relationship between training size and test accuracy. Test accuracy improves with increasing training dataset size. </div> </div> </div> <p> <strong>Training on moderate-sized datasets achieves strong code generation performance.</strong> Notably, the model displays zero accuracy without fine-tuning (<em>i.e., if we directly evaluate the pre-trained model on the test set without further fine-tuning on our generated dataset</em>). As shown in the figure to the right, when the training dataset size increases, the model's accuracy improves significantly from 0.695 at a training size of 600 to 0.95 at 5,000, which indicates a clear positive correlation between training size and test accuracy. This progression highlights the critical role of fine-tuning in model performance, with larger training datasets significantly enhancing the modelâ€™s code generation capabilities, underscoring the adaptability of pre-trained language models. </p> <div class="row"> <div class="col-md-6 text-center"> <figure> <picture> <img src="/assets/img/text2latex/depth_acc.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Performance variation with training set complexity. Illustrates model efficacy on test sets (tree depth 2) across varying complexities of training datasets, highlighting superior performance with increased complexity.</div> </div> <div class="col-md-6 text-center"> <figure> <picture> <img src="/assets/img/text2latex/txtvsimage.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Comparison of Text2LaTeX and Image2LaTeX model performance by varying training sizes. The model with augmented visual input exhibit consistently lower performance compared to that with text only inputs.</div> </div> </div> <p> <strong>Complexity boosts training effectiveness up to a point.</strong> We explored how training set depth affects model success. Models were trained on sets of 2500 samples and tested on a consistent set. As indicated by the figure to the left, accuracy topped at depth 3 (0.99) but dipped slightly at depth 4 (0.985), following an increase from depth 2 (0.885). This implies there is a point where adding complexity ceases to yield significant performance improvements. Pinpointing this balance is key to refining training and maximizing model results. </p> <p> <strong>Additional visual information doesn't always enhance performance.</strong> Investigating if textual data alone suffices for LaTeX format comprehension, we compared Text2LaTeX and Image2LaTeX models. The key difference lies in Image2LaTeX's extra image input, transformed into visual tokens. Contrary to expectations, adding visual inputs led to a drop in accuracy, as shown by the figure to the left. This suggests visual data might actually distract from processing LaTeX expressions. While a Vision Transformer (ViT) breaks down image inputs into patches, language models seem to manage string inputs more effectively for complex LaTeX expressions. </p> </section> </body> </html>]]></content><author><name></name></author><category term="blockquotes"/><category term="course-projects"/><category term="ML"/><category term="code"/><summary type="html"><![CDATA[learning-based LaTeX code generation]]></summary></entry><entry><title type="html">Exploring Group Distributionally Robust Optimization in Machine Learning - A Stochastic Optimization Perspective</title><link href="https://jiayuww.github.io/blog/2023/gdro-benders/" rel="alternate" type="text/html" title="Exploring Group Distributionally Robust Optimization in Machine Learning - A Stochastic Optimization Perspective"/><published>2023-12-16T10:18:00+00:00</published><updated>2023-12-16T10:18:00+00:00</updated><id>https://jiayuww.github.io/blog/2023/gdro-benders</id><content type="html" xml:base="https://jiayuww.github.io/blog/2023/gdro-benders/"><![CDATA[<section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="https://github.com/jiayuww/gdro-benders/blob/main/cs719_GDRO_JiayuWang.pdf" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Paper</span> </a> </span> <span class="link-block"> <a href="https://github.com/jiayuww/gdro-benders" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span> <span class="link-block"> <a href="https://github.com/jiayuww/gdro-benders" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="far fa-images"></i> </span> <span>Data</span> </a> </div> </div> </div> </div> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <div class="content has-text-justified"> <p> In this work, we explore binary classification problem under spurious correlations (prior shifts from the training to the test distribution). We discuss the formulation of four potential solutions and empirically examine the effectiveness of three methods: ERM with SGD, GDRO with Online Optimization, and GDRO with Benders Decomposition. </p> <p> Our findings reveal that both Online Optimization and Benders Decomposition excel under strong spurious correlations, with Benders Decomposition slightly outperforming Online Optimization. However, ERM still demonstrates effective performance under weak spurious correlation. In terms of computational costs, Online Optimization is significantly more demanding compared to other methods. We also observe that both ERM and Online Optimization are sensitive to learning rate adjustments. Benders Decomposition, on the other hand, is insensitive w.r.t. key parameters (\(\delta\) and \(C\)). Consequently, when convexity conditions are satisfied, Benders Decomposition offers a more stable and time-efficient solution than Online Optimization and ERM under strong spurious correlation. </p> </div> </div> </div> <section class="section" id="Main Results"> <h2 class="title">Main Results</h2> <div class="row mt-3"> <div class="col-md-6 mt-3"> <figure> <picture> <img src="/assets/img/train_200_cov2_231205-0052.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> (a) Training Set (\(\rho = 0.8\)) </div> </div> <div class="col-md-6 mt-3"> <figure> <picture> <img src="/assets/img/test_200_cov2_231205-2108_model-erm_bc_lr-0.4_mo-0.9_wd-0.0001_train-231205-0052_epoch-30.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> (b) ERM </div> </div> </div> <div class="row mt-3"> <div class="col-md-6 mt-3"> <figure> <picture> <img src="/assets/img/test_200_cov2_231205-2108_model-online_lr-0.2_mo-0.9_wd-0.0001_train-231205-0052.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> (c) OnlineOpt </div> </div> <div class="col-md-6 mt-3"> <figure> <picture> <img src="/assets/img/test_200_cov2_231205-2108_model-benders_delta-0.0001_C-50_train-231205-0052.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> (d) Benders </div> </div> </div> <div class="caption"> Visualization of decision boundaries. We consider binary classification with four groups. Figures (a) to (d) display training and test sets with respective decision boundaries for ERM, OnlineOpt, and Benders. The red line represents the decision boundary where the model predicts \(y=1\) above the line and \(y=0\) otherwise. Horizontal lines indicate desirable classifiers while vertical lines indicate reliance on spurious correlations. </div> <table style="width: 100%; border-collapse: collapse;" border="1"> <caption> Comparison of three methods in binary classification by varying degrees of spurious correlation \(\rho\) from 0.5 (no correlation) to 0.9 (strong correlation). We report average and worst-group accuracies on the same test set. The highest accuracies are marked in #fff3b0. </caption> <thead> <tr> <th rowspan="2">Method</th> <th colspan="2">\(\rho = 0.5\) (unbiased)</th> <th colspan="2">\(\rho = 0.6\)</th> <th colspan="2">\(\rho = 0.7\)</th> <th colspan="2">\(\rho = 0.8\)</th> <th colspan="2">\(\rho = 0.9\)</th> </tr> <tr> <th>avg acc</th> <th>worst acc</th> <th>avg acc</th> <th>worst acc</th> <th>avg acc</th> <th>worst acc</th> <th>avg acc</th> <th>worst acc</th> <th>avg acc</th> <th>worst acc</th> </tr> </thead> <tbody> <tr> <td>ERM</td> <td>0.895</td> <td style="background-color: #fff3b0;">0.890</td> <td>0.905</td> <td style="background-color: #fff3b0;">0.880</td> <td>0.890</td> <td>0.790</td> <td>0.865</td> <td>0.760</td> <td>0.860</td> <td>0.750</td> </tr> <tr> <td>OnlineOpt</td> <td>0.905</td> <td>0.850</td> <td>0.905</td> <td style="background-color: #fff3b0;">0.880</td> <td>0.905</td> <td>0.860</td> <td style="background-color: #fff3b0;">0.905</td> <td>0.830</td> <td>0.885</td> <td style="background-color: #fff3b0;">0.900</td> </tr> <tr> <td>Benders</td> <td style="background-color: #fff3b0;">0.910</td> <td>0.870</td> <td style="background-color: #fff3b0;">0.910</td> <td style="background-color: #fff3b0;">0.880</td> <td style="background-color: #fff3b0;">0.910</td> <td style="background-color: #fff3b0;">0.870</td> <td style="background-color: #fff3b0;">0.905</td> <td style="background-color: #fff3b0;">0.880</td> <td style="background-color: #fff3b0;">0.890</td> <td style="background-color: #fff3b0;">0.900</td> </tr> </tbody> </table> <p> <strong>OnlineOpt and Benders excel under stronger spurious correlations.</strong> Across scenarios with varying <span class="math">\(\rho\)</span>, both OnlineOpt and Benders generally outperform or match ERM in terms of average and worst-group accuracy, except in cases with minimal prior shifts from the training to the test distribution. Notably, the performance gap between ERM and the other two methods widens as <span class="math">\(\rho\)</span> increases, indicating a decrease in ERM's robustness under stronger spurious correlations. </p> <p> <strong>Benders marginally surpasses OnlineOpt across all settings.</strong> OnlineOpt and Benders demonstrate similar performance in terms of average and worst-group accuracies. However, Benders exhibits a slight advantage over OnlineOpt consistently in all the tasks evaluated. </p> <div class="row mt-3"> <div class="col-md-12 mt-3"> <figure> <picture> <img src="/assets/img/time.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Total time (seconds) of three methods under different degrees of spurious correlation \(\rho\). </div> </div> </div> <p> <strong>OnlineOpt incurs the highest time complexity.</strong> We calculate the total training time for methods under various <span class="math">\(\rho\)</span> levels. We can see that ERM is the most computationally efficient one, consistently outperforming OnlineOpt and Benders across various <span class="math">\(\rho\)</span> levels. OnlineOpt incurs the highest computational duration, which suggests its substantial complexity. Benders is more efficient than OnlineOpt but still lags behind ERM, particularly when spurious correlation intensifies. This highlights the trade-off between computational efficiency and robustness against varying degrees of spurious correlations for these methods. </p> </section> <section class="section" id="Ablation Studies"> <h2 class="title">Ablation Studies</h2> <div class="row"> <div class="col-md-6 text-center"> <figure> <picture> <img src="/assets/img/lr_erm.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Accuracies of ERM for different learning rates</div> </div> <div class="col-md-6 text-center"> <figure> <picture> <img src="/assets/img/lr_onlineopt.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Accuracies of OnlineOpt for different learning rates</div> </div> </div> <div class="caption text-center"> Accuracy comparison of ERM vs OnlineOpt for different learning rates. OnlineOpt outperforms ERM across different learning rates in general. ERM is more sensitive to learning rate than Online Optimization. </div> <p> <strong> Sensitivity of ERM and OnlineOpt to learning rate. </strong> 1) ERM is sensitive to changes in the learning rate, exhibiting notable variations in both average and worst group accuracies across different rates. 2) In contrast, OnlineOpt demonstrates considerably less sensitivity to learning rate adjustments, with the exception of a very small learning rate (0.0001). This suggests an advantage of OnlineOpt, as it does not require as meticulous a calibration of the learning rate compared to ERM. </p> <div class="row"> <div class="col-md-6 text-center"> <figure> <picture> <img src="/assets/img/bs_erm.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Accuracies of ERM for different batch sizes</div> </div> <div class="col-md-6 text-center"> <figure> <picture> <img src="/assets/img/bs_onlineopt.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Accuracies of OnlineOpt for different batch sizes</div> </div> </div> <div class="caption text-center"> Comparative analysis of ERM and OnlineOpt across various batch sizes. Both methods exhibit robustness to batch size variations. ERM demonstrates a more versatile range in batch size adaptability, in contrast to OnlineOpt constrained by the size of the smallest group. </div> <p> ERM and OnlineOpt are stable across various batch sizes. </p> <div class="row"> <div class="col-md-6 text-center"> <figure> <picture> <img src="/assets/img/delta_benders.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Accuracies of Benders for different \(\delta\)</div> </div> <div class="col-md-6 text-center"> <figure> <picture> <img src="/assets/img/c_benders.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Accuracies of Benders for different \(C\)</div> </div> </div> <p> Benders is robust to parameters <span class="math">\(\delta\)</span> and <span class="math">\(C\)</span>, which are the only parameters in this method. </p> </section> </body> </html>]]></content><author><name></name></author><category term="blockquotes"/><category term="course-projects"/><category term="optimization"/><category term="ML"/><summary type="html"><![CDATA[leveraging benders decomposition to solve group distributionally robust optimization]]></summary></entry></feed>