---
layout: post
date: 2025-6-5 12:59:00-0400
inline: true
related_posts: true
---
ðŸš€ **SPARKLE** preprint is now live on [arXiv](https://arxiv.org/abs/2506.04723)! 
Reinforcement learning has driven impressive gains in LLM reasoningâ€”but *what exactly* does RL improve? SPARKLE answers this question with a fine-grained evaluation framework that dissects reasoning into plan-following, problem decomposition, and knowledge use.

The results are surprising: explicit plans can actually hurt on the hardest problems, yet RL-tuned models remain far more robust and flexible in handling them. We also find clear gains in how RL enhances knowledge integration.

And we push back on a common myth: hard problems *can* be useful for RLâ€”even when they seem unrewarding. SPARKLE shows how to turn those tough cases into real training signal.